from workflows.textflows import *


def tokenizer_hub(input_dict):
    """
    Apply the *tokenizer* object on the Annotated Document Corpus (*adc*):

    1. first select only annotations of type *input_annotation*,
    2. apply the tokenizer
    3. create new annotations *output_annotation* with the outputs of the tokenizer.

    :param adc: Annotated Document Corpus (workflows.textflows.DocumentCorpus)
    :param tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    :param input_annotation: Which annotated part of document to be splitted.
    :param output_annotation: How to annotate the newly discovered tokens.

    :returns adc: Annotated Document Corpus (workflows.textflows.DocumentCorpus)
    """
    tokenizer_dict = input_dict['tokenizer']


    if type(tokenizer_dict)!=dict:
        from ...latino.library_gen import latino_tokenize_words
        return latino_tokenize_words(input_dict)
    else:
        tokenizer=tokenizer_dict['object']
        args=tokenizer_dict.get('args',[])
        kwargs=tokenizer_dict.get('kargs',{})
        input_annotation = input_dict['input_annotation']
        output_annotation = input_dict['output_annotation']
        adc = input_dict['adc']
        for document in adc.documents:
            if document.features['contentType'] == "Text":
                if not document.text:
                    pass
                for annotation,subtext in document.get_annotations_with_text(input_annotation): #all annotations of this type
                    new_token_spans=tokenizer.span_tokenize(subtext,*args,**kwargs)
                    for starts_at,ends_at in new_token_spans:
                        document.annotations.append(Annotation(annotation.span_start+starts_at,annotation.span_start+ends_at-1,output_annotation))
        return {'adc': adc}




def nltk_treebank_word_tokenizer(input_dict):
    raise NotImplementedError() #span_tokenize not implemented in this class

    import nltk
    return {'tokenizer': {'object': nltk.TreebankWordTokenizer()}}


def nltk_punkt_sentence_tokenizer(input_dict):
    """
    A sentence tokenizer which uses an unsupervised algorithm to build
    a model for abbreviation words, collocations, and words that start
    sentences; and then uses that model to find sentence boundaries.
    This approach has been shown to work well for many European
    languages.

    :returns tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """
    import nltk
    return {'tokenizer': {'object': nltk.PunktSentenceTokenizer()}}


def nltk_regex_tokenizer(input_dict):
    """
    The Regex Tokenizer splits a string into substrings using a regular expression.

    :param :param pattern: The pattern used to build this tokenizer.
        (This pattern may safely contain capturing parentheses.)
    :param gaps: True if this tokenizer's pattern should be used
        to find separators between tokens; False if this
        tokenizer's pattern should be used to find the tokens
        themselves.
    :param discard_empty: True if any empty tokens `''`
        generated by the tokenizer should be discarded.  Empty
        tokens can only be generated if `_gaps == True`.

    :return: tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """

    pattern = input_dict[u'pattern']
    gaps = input_dict[u'gaps'] == "true"
    discard_empty = input_dict[u'discard_empty'] == "true"
    #print pattern, gaps, discard_empty

    import nltk
    return {'tokenizer': {'object': nltk.RegexpTokenizer(pattern=pattern, gaps=gaps, discard_empty=discard_empty)}}


def nltk_sexpression_tokenizer(input_dict):
    """
    A tokenizer that divides strings into s-expressions.
    An s-expresion can be either:
      - a parenthesized expression, including any nested parenthesized
        expressions, or
      - a sequence of non-whitespace non-parenthesis characters.

    :param parens: A two-element sequence specifying the open and close parentheses
        that should be used to find sexprs.  This will typically be either a
        two-character string, or a list of two strings.
    :param strict: If true, then raise an exception when tokenizing an ill-formed sexpr.

    :return: tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """
    parens = input_dict[u'parens']
    strict = input_dict[u'strict'] == "true"
    #print parens, strict

    import nltk
    return {'tokenizer': {'object': nltk.SExprTokenizer(parens=parens, strict=strict)}}


def nltk_simple_tokenizer(input_dict):
    """
    These tokenizers divide strings into substrings using the string
    ``split()`` method.
    When tokenizing using a particular delimiter string, use
    the string ``split()`` method directly, as this is more efficient.

    :param (type unicode) u"char_tokenizer"
    :param (type unicode) u"space_tokenizer"
    :param (type unicode) u"tab_tokenizer"

    :return: tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """

    import nltk
    if input_dict["type"] == u"char_tokenizer":
        tokenizer = nltk.tokenize.simple.CharTokenizer()
    elif input_dict["type"] == u"space_tokenizer":
        tokenizer = nltk.SpaceTokenizer()
    elif input_dict["type"] == u"tab_tokenizer":
        tokenizer = nltk.TabTokenizer()

    return {'tokenizer': {'object': tokenizer}}


def nltk_line_tokenizer(input_dict):
    """
    Tokenize a string into its lines, optionally discarding blank lines.
    This is similar to ``s.split('\n')``

    :param blanklines: 'discard', 'keep', 'discard-eof'
    :return: tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """

    import nltk
    blanklines = input_dict[u"blanklines"]
    return {'tokenizer': {'object': nltk.LineTokenizer(blanklines=blanklines)}}


def nltk_stanford_tokenizer(input_dict):
    """
    Stanford Tokenizer

    :return: tokenizer: A python dictionary containing the Tokenizer object and its arguments.
    """

    from nltk.tokenize.stanford import StanfordTokenizer
    return {'tokenizer': {'object': StanfordTokenizer()}}




